# Databricks notebook source
# DBTITLE 1,import and initialize parameters
from datetime import datetime
import json

# Create a text widget to capture ADF parameters as a JSON string
dbutils.widgets.text("adf_params_dict", "")

# Parse the JSON string from the widget into a dictionary
adf_params_dict = json.loads(dbutils.widgets.get("adf_params_dict"))

# Record the start timestamp of the step
step_start_ts = datetime.now()

# Extract necessary parameters from the ADF parameters dictionary
Run_Id = adf_params_dict["run_id"]
Source_System = adf_params_dict["source_system"]
stg_table_name = adf_params_dict["curated_tbl"]

# Define the name of the current step
step_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().split('/')[-1]

# Extract the FDP flow statistics table name from the ADF parameters
fdp_flow_statistics = adf_params_dict["result_transcription_tables"]["fdp_flow_statistics"]

# Initialize an empty dictionary to store insert script data
insert_script_dict = {}

# COMMAND ----------

# DBTITLE 1,import logging functions
# MAGIC %run ../00_generic/00_insert_result_transcription_tables

# COMMAND ----------

# DBTITLE 1,define sql query
query = (f"""
INSERT INTO fdp.rdv.L_POSTING_RECORD_POSTING (L_Posting_Record_Posting_HK, Load_Dts, Run_Id, Invalid_Dts, Rec_Src, Source_System, H_Posting_Record_HK, H_Posting_HK)
SELECT DISTINCT
    md5(CONCAT(src.h_posting_hk, '_', src.h_posting_record_hk)) AS L_Posting_Record_Posting_HK,
    current_timestamp() AS Load_Dts,
    '{Run_Id}' AS Run_Id,
    '9999-12-31 23:59:59' AS Invalid_Dts,
    src.Rec_Src,
    '{Source_System}' AS Source_System,
    src.H_Posting_Record_HK,
    src.H_Posting_HK   
FROM 
    (SELECT 
        md5(stg.file_name) AS H_Posting_HK,
        md5(CONCAT(stg.file_name, '_', stg.line_number)) AS H_Posting_Record_HK, 
        CONCAT(stg.file_name, '~', stg.line_number) AS Rec_Src
    FROM 
        (SELECT 
            line_number,
            file_name
        FROM fdp.curated.`{stg_table_name}`) stg) src
LEFT JOIN fdp.rdv.L_POSTING_RECORD_POSTING tgt
    ON md5(CONCAT(src.h_posting_hk, '_', src.h_posting_record_hk)) = tgt.L_Posting_Record_Posting_HK
WHERE tgt.L_Posting_Record_Posting_HK IS NULL;
""")

# COMMAND ----------

# DBTITLE 1,execute the query
step_status = ""
step_error = ""
step_rows_write = 0

try:
    result = spark.sql(query)
    step_rows_write = result.first()['num_inserted_rows']
    step_status = "COMPLETED"
    step_error = None
except Exception as e:
    step_status = "FAILED"
    step_error = type(e).__name__
    raise e
finally:
    # Log status to FDP flow statistics table
    insert_script_dict.update({
        "run_id": adf_params_dict["run_id"],
        "run_date": step_start_ts.date(),
        "flow_name": adf_params_dict["flow_name"],
        "object_id": adf_params_dict["object_id"],
        "source_name": adf_params_dict["source_name"],
        "step_name": step_name,
        "step_source": adf_params_dict["step_source"],
        "step_status": step_status,
        "step_rows_write": step_rows_write,
        "step_error": step_error,
        "step_start_ts": step_start_ts,
        "step_end_ts": datetime.now(),
        "fdp_flow_statistics": fdp_flow_statistics,
    })
    insert_fdp_flow_statistics(insert_script_dict)
    if step_status == "FAILED":
        raise ValueError(f"Code Failed with error type:'{step_error}'")
