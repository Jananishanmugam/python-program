# Databricks notebook source
# DBTITLE 1,import and initialize parameters
from datetime import datetime
import json

# Create a text widget to capture ADF parameters as a JSON string
dbutils.widgets.text("adf_params_dict", "")

# Parse the JSON string from the widget into a dictionary
adf_params_dict = json.loads(dbutils.widgets.get("adf_params_dict"))

# Record the start timestamp of the step
step_start_ts = datetime.now()

# Extract necessary parameters from the ADF parameters dictionary
Run_Id = adf_params_dict["run_id"]
Source_System = adf_params_dict["source_system"]
stg_table_name = adf_params_dict["curated_tbl"]

# Define the name of the current step
step_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().split('/')[-1]

# Extract the FDP flow statistics table name from the ADF parameters
fdp_flow_statistics = adf_params_dict["result_transcription_tables"]["fdp_flow_statistics"]

# Initialize an empty dictionary to store insert script data
insert_script_dict = {}

# COMMAND ----------

# DBTITLE 1,import logging functions
# MAGIC %run ../00_generic/00_insert_result_transcription_tables

# COMMAND ----------

# DBTITLE 1,define sql query
query = (f"""
INSERT INTO fdp.rdv.H_POSTING_RECORD (H_Posting_Record_Hk, Load_Dts, Invalid_Dts, Run_Id, Rec_Src, Source_System, Posting_Record_Bk)
SELECT DISTINCT
	md5(src.Posting_Record_Bk) AS H_Posting_Record_Hk,
	current_timestamp() AS Load_Dts,
    '9999-12-31 23:59:59' AS Invalid_Dts, 
    '{Run_Id}' AS Run_Id,
    src.Rec_Src, 
   '{Source_System}' AS Source_System,
    src.Posting_Record_Bk
FROM (SELECT 
			   concat(stg.file_name,'~', stg.line_number) AS Rec_Src,
			   concat(stg.file_name,'_', stg.line_number) AS Posting_Record_Bk
			FROM 
				(	SELECT 
					 line_number,
					 file_name
					 FROM fdp.curated.`{stg_table_name}`) stg ) src
LEFT JOIN fdp.rdv.H_POSTING_RECORD AS tgt
    ON md5(src.Posting_Record_Bk) = tgt.H_Posting_Record_Hk
WHERE tgt.H_Posting_Record_Hk IS NULL;
""")

# COMMAND ----------

# DBTITLE 1,execute the query
step_status = ""
step_error = ""
step_rows_write = 0

try:
    result = spark.sql(query)
    step_rows_write = result.first()['num_inserted_rows']
    step_status = "COMPLETED"
    step_error = None
except Exception as e:
    step_status = "FAILED"
    step_error = type(e).__name__
    raise e
finally:
    # Log status to FDP flow statistics table
    insert_script_dict.update({
        "run_id": adf_params_dict["run_id"],
        "run_date": step_start_ts.date(),
        "flow_name": adf_params_dict["flow_name"],
        "object_id": adf_params_dict["object_id"],
        "source_name": adf_params_dict["source_name"],
        "step_name": step_name,
        "step_source": adf_params_dict["step_source"],
        "step_status": step_status,
        "step_rows_write": step_rows_write,
        "step_error": step_error,
        "step_start_ts": step_start_ts,
        "step_end_ts": datetime.now(),
        "fdp_flow_statistics": fdp_flow_statistics,
    })
    insert_fdp_flow_statistics(insert_script_dict)
    if step_status == "FAILED":
        raise ValueError(f"Code Failed with error type:'{step_error}'")
