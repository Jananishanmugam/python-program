# Databricks notebook source
# DBTITLE 1,Import ADF Flow Configs
# MAGIC %run ../00_configs/00_adf_orchestration/00_fdp_adf_flow_params_config

# COMMAND ----------

# DBTITLE 1,Import FDP Logger
# MAGIC %run ../00_logs/00_fdp_logger

# COMMAND ----------

# DBTITLE 1,Initialize Databricks Parameters
import yaml
import json
from datetime import datetime
from collections import OrderedDict
from pyspark.sql.functions import col, first
import re

# Retrieve ADF parameters passed to the notebook
dbutils.widgets.text("interface_file_name", "")
interface_file_name = dbutils.widgets.get("interface_file_name")
dbutils.widgets.text("interface_folder_path", "")
interface_folder_path = dbutils.widgets.get("interface_folder_path")

# Define the base path for YAML output
yaml_path = config_paths["base_path_yaml"]

# Get the current date
today_date = datetime.today()

# Filter and obtain the object name from the interface file name
object_name = interface_file_name.rsplit(".", 1)[0]

# Extract and uppercase the interface name from the folder path
interface_name = interface_folder_path.split("/")[-1].upper()

# Remove the file extension from the interface file name
interface_file_name_no_ext = interface_file_name.rsplit(".", 1)[0]

# Get the ADF configuration for the given interface name and file name
adf_config = get_file_config_by_regex(interface_name, interface_file_name_no_ext)

# Define the DQ YAML file name from the ADF configuration
object_yaml = f"{object_name}_dq_check.yaml"
dq_yaml_file = adf_config.get("standard_dq_yaml", object_yaml)

# Define the log directory and file path for logging
LOG_DIR = f"logs/{today_date.strftime('%Y%m%d')}/dq_yaml_builder_log/{object_name}"
adls_log_path = f"{file_paths['log_path']}/{LOG_DIR}"

# Initialize the logger with the log path and current timestamp
logger = Logger(adls_log_path, f"{today_date.strftime('%Y%m%d%H%M%S')}.log")

# COMMAND ----------

# DBTITLE 1,Retrive Object ID
# Retrieve file name patterns from the data object catalog table
df = spark.sql(f"SELECT file_name_pattern FROM {knowledge_base_tables['data_object_catalog']}")

# Collect file name patterns into a list
file_name_pattern_list = [row.file_name_pattern for row in df.collect()]

# Find the matching file pattern for the interface file name
file_pattern = next(
    (pattern for pattern in file_name_pattern_list if re.match(pattern, interface_file_name_no_ext)),
    None,
)

# Retrieve the object ID from the data object catalog table
if file_pattern is None:
    raise ValueError(f"No file pattern found for {interface_file_name}")
else:
    object_id = spark.sql(
        f"""
        SELECT object_id
        FROM {knowledge_base_tables['data_object_catalog']}
        WHERE file_name_pattern = '{file_pattern}'
        """
    ).first()["object_id"]

# COMMAND ----------

# DBTITLE 1,Main
try:
    # Remove existing DQ YAML file if it exists
    dbutils.fs.rm(f"{yaml_path}/{object_name}_dq_check.yaml")
    logger.log_info("Existing DQ YAML file removed...")

    # Load DataFrames with error handling
    logger.log_info("Loading data from tables...")

    # Load and cache the mapping table DataFrame
    mapping_df = spark.sql(f"SELECT * FROM {knowledge_base_tables['mapping_table']}").cache()

    # Load and cache the control dictionary table DataFrame
    control_df = spark.sql(f"SELECT * FROM {knowledge_base_tables['control_dictionary']}").cache()

    # Load and cache the data object catalog table DataFrame
    entity_df = spark.sql(f"SELECT * FROM {knowledge_base_tables['data_object_catalog']}").cache()

    # Load and cache the data instance catalog table DataFrame
    instance_df = spark.sql(f"SELECT * FROM {knowledge_base_tables['data_instance_catalog']}").cache()

    # Log the number of rows loaded from each table
    logger.log_info(f"Loaded {mapping_df.count()} rows from mapping_table")
    logger.log_info(f"Loaded {control_df.count()} rows from control_dictionary")
    logger.log_info(f"Loaded {entity_df.count()} rows from data_object_catalog")
    logger.log_info(f"Loaded {instance_df.count()} rows from data_instance_catalog")

    # Filter control dictionary to include only active data controls excluding uniqueness checks
    control_filtered_df = control_df.filter(
        (col("control_object") == "Data") &
        (col("control_name") != "Uniqueness Check") &
        (col("active_flag") == "Y")
    )
    logger.log_info("Filtered control dictionary successfully.")

    # Join mapping table with filtered control dictionary and entity table
    mapping_entity_df = (
        mapping_df.join(control_filtered_df, "control_id", "inner")
        .join(entity_df, mapping_df["data_object_id"] == entity_df["object_id"], "inner")
        .select(
            "mapping_id", "control_id", "object_id", "data_instance_id",
            "control_name", "technical_function"
        )
    ).filter(col("object_id") == object_id)

    # Join the resulting DataFrame with instance table and select relevant columns
    final_mapping_df = mapping_entity_df.join(instance_df, "data_instance_id", "left").select(
        col("mapping_id"), col("control_id"), col("object_id"), col("data_instance_name"),
        col("control_name"), col("technical_function"), col("field_length"),
        col("data_syntax"), col("field_format")
    )
    logger.log_info(f"Final mapping DataFrame has {final_mapping_df.count()} rows")

    # Create DataFrame for uniqueness checks
    dup_check_final_df = (
        mapping_df.join(
            control_df.filter(
                (col("control_name") == "Uniqueness Check") &
                (col("active_flag") == "Y")
            ), "control_id"
        )
        .join(entity_df, mapping_df["data_object_id"] == entity_df["object_id"], "inner")
        .join(instance_df, "data_instance_id", "left")
        .select(
            col("mapping_id"), col("control_id"), col("object_id"), col("data_instance_name")
        )
    ).filter(col("object_id") == object_id)

    # Aggregate uniqueness check columns
    dup_check_column_df = dup_check_final_df.groupBy("object_id").agg(
        first("data_instance_name").alias("dup_column"),
        first("mapping_id").alias("dup_mapping_id"),
        first("control_id").alias("dup_control_id")
    )

    # Join final mapping DataFrame with uniqueness check columns
    final_df = final_mapping_df.join(dup_check_column_df, "object_id", "left")
    logger.log_info(f"Final DataFrame after joining uniqueness checks has {final_df.count()} rows")

    # Initialize YAML data structure
    yaml_data = OrderedDict()
    yaml_data["data_quality_check"] = OrderedDict()
    yaml_data["data_quality_check"]["reject_reason"] = True
    yaml_data["data_quality_check"]["check_columns"] = []

    # Process each row in the final DataFrame to populate YAML data
    for row in final_df.collect():
        column_entry = OrderedDict()
        column_entry["column"] = row["data_instance_name"]
        column_entry["dq_checks"] = row["technical_function"]

        # Add field length if applicable
        if row["technical_function"] == "field_length" and row["field_length"]:
            column_entry["length"] = int(row["field_length"])

        # Add data syntax and format if applicable
        if row["technical_function"] == "formats":
            column_entry["type"] = row["data_syntax"].lower() if row["data_syntax"] else None
            if row["data_syntax"] and row["data_syntax"].upper() == "DECIMAL" and row["field_format"]:
                precision_scale = row["field_format"].split(",")
                if len(precision_scale) == 2:
                    column_entry["precision"] = int(precision_scale[0].strip())
                    column_entry["scale"] = int(precision_scale[1].strip())
            else:
                column_entry["format"] = row["field_format"]

        column_entry["mapping_id"] = row["mapping_id"]
        column_entry["control_id"] = row["control_id"]

        yaml_data["data_quality_check"]["check_columns"].append(column_entry)

    # Add uniqueness check column if present
    first_dup = final_df.first()
    if first_dup["dup_column"]:
        yaml_data["data_quality_check"]["dup_check"] = first_dup["dup_column"]
        yaml_data["data_quality_check"]["mapping_id"] = first_dup["dup_mapping_id"]
        yaml_data["data_quality_check"]["control_id"] = first_dup["dup_control_id"]

    # Convert YAML data to string and save to file
    yaml_data = json.loads(json.dumps(yaml_data))
    yaml_string = yaml.dump(yaml_data, default_flow_style=False, sort_keys=False)
    yaml_output_path = f"{yaml_path}/{dq_yaml_file}"

    try:
        dbutils.fs.put(yaml_output_path, yaml_string, overwrite=True)
        logger.log_info(f"Successfully wrote YAML file: {yaml_output_path}")
    except Exception as e:
        logger.log_error(f"Failed to write YAML file for object {object_name}: {e}")
        raise e

except Exception as e:
    logger.log_error(f"Unexpected error in processing yaml generator: {e}")
    raise e

finally:
    # Write logs to ADLS
    logger.write_logs_to_adls()
