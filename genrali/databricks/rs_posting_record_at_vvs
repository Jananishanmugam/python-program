# Databricks notebook source
# DBTITLE 1,import and initialize parameters
from datetime import datetime
import json

# Create a text widget to capture ADF parameters as a JSON string
dbutils.widgets.text("adf_params_dict", "")

# Parse the JSON string from the widget into a dictionary
adf_params_dict = json.loads(dbutils.widgets.get("adf_params_dict"))

# Record the start timestamp of the step
step_start_ts = datetime.now()

# Extract necessary parameters from the ADF parameters dictionary
Run_Id = adf_params_dict["run_id"]
Source_System = adf_params_dict["source_system"]
stg_table_name = adf_params_dict["curated_tbl"]

# Define the name of the current step
step_name = dbutils.notebook.entry_point.getDbutils().notebook().getContext().notebookPath().get().split('/')[-1]

# Extract the FDP flow statistics table name from the ADF parameters
fdp_flow_statistics = adf_params_dict["result_transcription_tables"]["fdp_flow_statistics"]

# Initialize an empty dictionary to store insert script data
insert_script_dict = {}

# COMMAND ----------

# DBTITLE 1,import logging functions
# MAGIC %run ../00_generic/00_insert_result_transcription_tables1

# COMMAND ----------

# DBTITLE 1,define sql query
query = (
    f"""
INSERT INTO fdp.rdv.RS_POSTING_RECORD_AT_VVS (
  Load_Dts, 
  H_Posting_Record_Hk, 
  Invalid_Dts,
	Run_Id,
	Source_System,
	Rec_Src,
	Hash_Diff,
	Amount,
	Currency,
	Posting_Direction,
	Posting_Dt,
	Assigment_Field,
  Local_Account_Nbr,
	Posting_Item_Desc,
	Reference_Key_3,
  Source_File_Name,
  Source_File_Line_Nbr)
SELECT DISTINCT
  current_timestamp() AS Load_Dts,
  src.H_Posting_Record_Hk,
  '9999-12-31 23:59:59' AS Invalid_Dts,
  '{Run_Id}' AS Run_Id,
  '{Source_System}' AS Source_System,
  src.Rec_Src,  
  src.Hash_Diff,
  src.Amount,
  src.Currency,
  src.Posting_Direction,
  src.Posting_Dt,
  src.Assignment_Field,
  src.Local_Account_Nbr,
  src.Posting_Item_Desc,
  src.Reference_Key_3,
  src.Source_File_Name,
  src.Source_File_Line_Nbr
  FROM
  (
   SELECT 
        md5(concat(stg.Source_File_Name,'_', stg.Source_File_Line_Nbr)) AS H_Posting_Record_Hk,
        concat(stg.Source_File_Name,'_', stg.Source_File_Line_Nbr) AS Rec_Src,
        md5(CONCAT(NVL(stg.Amount,'~Null~'),
          NVL(stg.Currency,'~Null~'),
          NVL(stg.Posting_Direction,'~Null~'),
          NVL(stg.Posting_Dt,'1900-01-01'), 
          NVL(stg.Assignment_Field,'~Null~'),
          NVL(stg.Local_Account_Nbr,'~Null~'),
          NVL(stg.Posting_Item_Desc,'~Null~')
        )) AS Hash_Diff,
        stg.Amount,
        stg.Currency,
        stg.Posting_Direction,
        stg.Posting_Dt,
        stg.Assignment_Field,
        stg.Local_Account_Nbr,
        stg.Posting_Item_Desc,
        stg.Reference_Key_3,
        stg.Source_File_Name,
        stg.Source_File_Line_Nbr  
   FROM 
		(SELECT
         CAST(TRIM(REPLACE(REPLACE(WRBTR, ',', '.'), '/', '')) AS DECIMAL(15,2)) AS Amount,
         TRIM(REPLACE(WAERS,'/','')) AS Currency,
         TRIM(REPLACE(NEWBS,'/','')) AS Posting_Direction,
         to_date(TRIM(REPLACE(BUDAT,'/','')), 'ddMMyyyy') AS Posting_Dt,
         TRIM(REPLACE(ZUONR,'/','')) AS Assignment_Field,
         TRIM(REPLACE(NEWKO,'/','')) AS Local_Account_Nbr,
         TRIM(REPLACE(SGTXT,'/','')) AS Posting_Item_Desc,
         TRIM(REPLACE(XREF3,'/','')) AS Reference_Key_3,
         file_name AS Source_File_Name,
         line_number AS Source_File_Line_Nbr
		FROM fdp.curated.`{stg_table_name}`
	  ) stg
			) src
LEFT OUTER JOIN fdp.rdv.RS_POSTING_RECORD_AT_VVS tgt
ON src.H_Posting_Record_Hk= tgt.H_Posting_Record_Hk
WHERE tgt.H_Posting_Record_Hk IS NULL
OR (tgt.H_Posting_Record_Hk  iS NOT NULL AND src.Hash_Diff <> tgt.Hash_Diff)
"""
)

# COMMAND ----------

# DBTITLE 1,execute the query
step_status = ""
step_error = ""
step_rows_write = 0

try:
    result = spark.sql(query)
    step_rows_write = result.first()['num_inserted_rows']
    step_status = "COMPLETED"
    step_error = None
except Exception as e:
    step_status = "FAILED"
    step_error = type(e).__name__
    raise e
finally:
    # Log status to FDP flow statistics table
    insert_script_dict.update({
        "run_id": adf_params_dict["run_id"],
        "run_date": step_start_ts.date(),
        "flow_name": adf_params_dict["flow_name"],
        "object_id": adf_params_dict["object_id"],
        "source_name": adf_params_dict["source_name"],
        "step_name": step_name,
        "step_source": adf_params_dict["step_source"],
        "step_status": step_status,
        "step_rows_write": step_rows_write,
        "step_error": step_error,
        "step_start_ts": step_start_ts,
        "step_end_ts": datetime.now(),
        "fdp_flow_statistics": fdp_flow_statistics,
    })
    insert_fdp_flow_statistics(insert_script_dict)
    if step_status == "FAILED":
        raise ValueError(f"Code Failed with error type:'{step_error}'")
